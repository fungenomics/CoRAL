---
title: "scCoAnnotate - Benchmarking"
output:
  html_document:
    df_print: paged
    theme: flatly
    toc: yes
    toc_float: yes
    toc_depth: 1 
    code_folding: hide
params: 
    tools: ''
    min_agree: ''
    ref_name: ''
    pred_path: ''
    fold: ''
    accuracy_metric: ''
    ontology_path: ''
    ontology_columns: ''
    wd: !r getwd()
---

```{r, echo=FALSE}
knitr::opts_chunk$set(message = FALSE, warning=FALSE)
```

```{r, fig.show='hide', include=F}
set.seed(1234)
library(tidyverse)
# library(caret)
library(ComplexHeatmap)
library(glue)
library(plotly)
source(paste0(dirname(params$wd), '/Scripts/Functions/functions.R'))

#empty plotly plot to make sure the other plotly plots get printed later 
plotly_empty() 
```

```{r}
min_agree = strsplit(params$min_agree, split = ' ')[[1]]
tools = c(paste0('Consensus_',min_agree), strsplit(params$tools, split = ' ')[[1]])
fold = as.numeric(params$fold)
accuracy_metric = params$accuracy_metric
ontology_columns = params$ontology_columns
ontology = data.table::fread(params$ontology_path,
                             sep = ",",
                             header = T) %>% as.data.frame()
```

```{r}
# Read prediction and true labels for each tool and each fold and calculate confusion matrix and stats 
# Save everything in a list object with hierarchy TOOL > FOLD > STATS 
list = list()

#Confusion matrix list
cm_list <- list()

for(n in 1:fold){
   
   # read tru lables 
   true = data.table::fread(paste0(params$pred_path, '/fold', n, '/test_labels.csv'), header = T) %>%
           column_to_rownames('V1') %>% 
     as.data.frame()

   true[,'label'] <- apply_ontology(df_ontology = ontology,
                                    pred = true[,'label',drop=T],
                                    from = 'label',
                                    to = ontology_columns)

   # true <- true %>% mutate(label = factor(label, ordered = TRUE)) 

   # read prediction summary for fold
   pred = data.table::fread(paste0(params$pred_path, '/fold', n, '/Prediction_Summary_',ontology_columns,'.tsv'), header = T)%>%
           column_to_rownames('cellname') %>% 
     as.data.frame()

   # make sure the predictions and true lables are in the same order
   pred = pred[rownames(true),]
   
   for(t in tools){
     # tmp = get_pred(pred, t, true)
     tmp = data.frame(class = true$label,
                      prediction = pred[,t,drop=T]
                      )

     # list[[t]][[n]] = confusionMatrix(data = tmp$label, reference = true$label, mode = 'everything')
     list[[t]][[n]] = get_metrics(tmp)
     list[[t]][[n]]$fold = paste0('fold', n)
     list[[t]][[n]]$tool = t
     cm_list[[t]][[n]] <- table(tmp$prediction,tmp$class)
     #change na values to 0 
     # list[[t]][[n]]$byClass[is.na(list[[t]][[n]]$byClass)] = 0
  }
}

# save list object with all stats 
save(list, file=paste0(params$pred_path, '/report/stats_',ontology_columns,'.Rda'))
save(cm_list, file=paste0(params$pred_path, '/report/confusion_matrix_',ontology_columns,'.Rda'))

# save accuracy metrics as table 
# accuracy_metric_res = lapply(list, function(x){lapply(x, get_all_stats) %>% bind_rows()}) %>% bind_rows()
accuracy_metric_res = list %>% bind_rows()
data.table::fwrite(accuracy_metric_res, file=paste0(params$pred_path, '/report/metrics_',ontology_columns,'.csv'))
```

```{r}
# Read training data class labels 

test_lab = list()
for(n in 1:fold){
  test_lab[[n]] = data.table::fread(paste0(params$pred_path, '/fold', n, '/test_labels.csv'), header = T) %>%
           column_to_rownames('V1') %>%
           mutate(label = factor(label, ordered = TRUE),
           fold = paste0('fold', n))
}

test_lab = bind_rows(test_lab)

test_lab$label <- apply_ontology(df_ontology = ontology,
                                    pred = as.character(test_lab$label),
                                    from = 'label',
                                    to = ontology_columns)
```

```{r echo=FALSE,message=FALSE,results="asis",fig.height=10,fig.width=10}
cat("  \n#", params$ref_name , "{.tabset} \n")

cat("  \n## Summary \n")
cat("<h3>Average metric score per tool and class</h3>") 

plot_mean_tool(list, accuracy_metric, tools, test_lab)

cat("\n")
cat("<h3>Average metric score per class</h3>") 
p = plot_performance_by_class(accuracy_metric_res,accuracy_metric)
print(htmltools::tagList(p))

cat("\n")
cat("<h3>Average metric score per tool</h3>") 
p = plot_performance_by_tool(accuracy_metric_res,accuracy_metric)
print(htmltools::tagList(p))

cat("\n")
for(t in tools) {
  cat("  \n##",  t, "{.tabset} \n")
  
  print(plot_stat_boxplot(list, t, accuracy_metric))
  
  cat("\n")
  
  for(n in 1:fold){
    cat("  \n###",  paste0('Fold ', n), " \n")
    
    cat("<h3>Confusion Matrix</h3>")
    
    draw(plot_cm(cm_list[[t]][[n]]))
  
    cat("<h3>Metric</h3>")
    
    print(plot_stat(list[[t]][[n]], accuracy_metric))
    
    cat("\n")
  }
}
```

# Report Info 

## Parameters
```{r echo=FALSE,message=FALSE,results="asis"}
for(p in names(params)){
  cat(" \n -",p,": ", params[[p]], " \n")
}
```

## Session  

```{r}
sessionInfo()
```




